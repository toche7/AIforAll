{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toche7/AIforAll/blob/main/word_embeddings_DATA_Short.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYwcIXq4t7cg"
      },
      "source": [
        "#Word embeddings in 2020. Review with code  examples\n",
        "\n",
        "by [Rostyslav Neskorozhenyi](https://www.linkedin.com/in/slanj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8IxcZQGTGb6"
      },
      "source": [
        "## Word2Vec and GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHdtizR4T1d_"
      },
      "source": [
        "The most commonly used models for word embeddings are [word2vec](https://github.com/dav/word2vec/) and [GloVe](https://nlp.stanford.edu/projects/glove/) which are both unsupervised approaches based on the distributional hypothesis (words that occur in the same contexts tend to have similar meanings).\n",
        "\n",
        "Word2Vec word embeddings are vector representations of words,\n",
        "that are typically learnt by an unsupervised model when fed\n",
        "with large amounts of text as input (e.g. Wikipedia, science, news, articles etc.). These representation of words capture semantic similarity between words among other properties. Word2Vec word embeddings are learnt in a such way, that [distance](https://en.wikipedia.org/wiki/Euclidean_distance) between vectors for words with close meanings (\"king\" and \"queen\" for example) are closer than distance for words with complety different meanings (\"king\" and \"carpet\" for example).\n",
        "\n",
        "![Замещающий текст](https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg)\n",
        "Image from [developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)\n",
        "\n",
        "Word2Vec vectors even allow some mathematic operations on vectors. For example, in this operation we are using word2vec vectors for each word:\n",
        "\n",
        "**king - man + woman = queen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEFSugKAFXeB"
      },
      "source": [
        "Another word embedding method is **Glove** (“Global Vectors”). It is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus. Then this matrix is factorized to a lower-dimensional (word x features) matrix, where each row now stores a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "wMk1XtkBS16q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27udJLwcbVLv"
      },
      "source": [
        "# Try Glove word embeddings with Spacy\n",
        "!python3 -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkURcwd5LJUp"
      },
      "source": [
        "import spacy\n",
        "# Load the spacy model that you have installed\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "# process a sentence using the model\n",
        "doc = nlp(\"man king stands on the carpet and sees woman queen\")\n",
        "# Get the vector for 'king':\n",
        "doc[1].vector[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc[1].vector[:])"
      ],
      "metadata": {
        "id": "n5h-wr8-rs8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(doc)):\n",
        "  print(doc[i])"
      ],
      "metadata": {
        "id": "P8Q0x1DqmE8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-998SUfAx-"
      },
      "source": [
        "Find similarity between King and Queen (higher value is better)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (i, dToken) in enumerate(doc):\n",
        "  print(i,dToken)"
      ],
      "metadata": {
        "id": "UrMvZ_gJ890Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K34HpkmWaEqV"
      },
      "source": [
        "doc[1].similarity(doc[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXm7aeh4hES5"
      },
      "source": [
        "Find similarity between King and carpet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSqwqBrUcwMB"
      },
      "source": [
        "doc[1].similarity(doc[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHsyuyS3h65C"
      },
      "source": [
        "Check if king - man + woman = queen. We will multiply vectors for 'man' and 'woman' by two, because subtracting the vector for 'man' and adding the vector for 'woman' will do little to the original vector for “king”, likely because those “man” and “woman” are related themselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXQ6uvkHh2bA"
      },
      "source": [
        "v =  doc[1].vector - (doc[0].vector*3) + (doc[8].vector*3)\n",
        "\n",
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "\n",
        "# Format the vocabulary for use in the distance function\n",
        "vectors = [token.vector for token in doc]\n",
        "vectors = np.array(vectors)\n",
        "\n",
        "# Find the closest word below\n",
        "closest_index = distance.cdist(np.expand_dims(v, axis = 0), vectors, metric = 'cosine').argmin()\n",
        "output_word = doc[closest_index].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjVLpbCG9829"
      },
      "source": [
        "output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uD3aN1wNDYM5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxnDjaFYMolw"
      },
      "source": [
        "## References\n",
        "\n",
        "- [BERT Word Embeddings Tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [The Illustrated GPT-2 (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/)\n",
        "- [FROM Pre-trained Word Embeddings TO Pre-trained Language Models — Focus on BERT](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)\n",
        "- [ Make your own Rick Sanchez (bot) with Transformers and DialoGPT fine-tuning](https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30)\n",
        "- [Playing with word vectors](https://medium.com/swlh/playing-with-word-vectors-308ab2faa519)\n",
        "- [Intuitive Guide to Understanding GloVe Embeddings](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)\n",
        "- [Word Embeddings in Python with Spacy and Gensim](https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/)\n",
        "- [Brief review of word embedding families (2019) ](https://medium.com/analytics-vidhya/brief-review-of-word-embedding-families-2019-b2bbc601bbfe)\n",
        "- [Word embeddings: exploration, explanation, and exploitation (with code in Python)](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)"
      ]
    }
  ]
}