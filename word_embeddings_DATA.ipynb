{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toche7/AIforAll/blob/main/word_embeddings_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYwcIXq4t7cg"
      },
      "source": [
        "#Word embeddings in 2020. Review with code  examples\n",
        "\n",
        "by [Rostyslav Neskorozhenyi](https://www.linkedin.com/in/slanj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uabj5jkUt_hK"
      },
      "source": [
        "In this article we will study word embeddings - digital representation of words suitable for processing by machine learning algorithms.\n",
        "\n",
        "Originally I created this article as a general overview and compilation of current approaches to word embeddings in 2020, which our [AI Labs](http://ai-labs.org/) team could use from time to time as a quick refresher. I hope that my article will be useful to a wider circle of data scientists and developers. Each word embedding method in the article has a (very) short description, links for further study, and code examples in Python. All code is packed as [Google Colab Notebook](https://colab.research.google.com/drive/1N7HELWImK9xCYheyozVP3C_McbiRo1nb). So let's begin.\n",
        "\n",
        "According to Wikipedia, **Word embedding** is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knWTPeFGvCVr"
      },
      "source": [
        "## One-hot or CountVectorizing\n",
        "\n",
        "The most basic method for transforming words into vectors is to count occurrence of each word in each document. Such approach is called countvectorizing or one-hot encoding.\n",
        "\n",
        "The main principle of this method is to collect a set of documents (they can be words, sentences, paragraphs or even articles) and count the occurrence of every word in each document. Strictly speaking, the columns of the resulting matrix are words and the rows are documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqL-rowBt3FB"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "          'Text of the very first new sentence with the first words in sentence.',\n",
        "          'Text of the second sentence.',\n",
        "          'Number three with lot of words words words.',\n",
        "          'Short text, less words.',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nz67PMDv4-1"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp5Ylyahv-Sx"
      },
      "source": [
        "# learn the vocabulary and store CountVectorizer sparse matrix in term_frequencies\n",
        "term_frequencies = vectorizer.fit_transform(corpus)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Wzj12tFU4-"
      },
      "source": [
        "term_frequencies = term_frequencies.toarray() # convert sparse matrix to numpy array\n",
        "term_frequencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AMSth6SJ-VL"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(term_frequencies, annot=True, cbar = False, xticklabels = vocab);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5swG8_LdwPEN"
      },
      "source": [
        "# Convert another document with countvectorizing\n",
        "vectorizer.transform(['A new new sentence.']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKO-WI3IEuaR"
      },
      "source": [
        "Another approach in countvectorizing is just to place 1 if the word is found in the document (no matter how often) and 0 if the word is not found in the document. In this case we get real 'one-hot' encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd8wwfuWED1w"
      },
      "source": [
        "one_hot_vectorizer = CountVectorizer(binary=True)\n",
        "one_hot = one_hot_vectorizer.fit_transform(corpus).toarray()\n",
        "one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMr0ixndJlL7"
      },
      "source": [
        "sns.heatmap(one_hot, annot=True, cbar = False, xticklabels = vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yiZ6Ua4w5u2"
      },
      "source": [
        "## TF-IDF encoding\n",
        "\n",
        "With a large corpus of documents some words like ‘a’, ‘the’, ‘is’, etc. occur very frequently but they don’t carry a lot of information. Using one-hot encoding approach we can decide that these words are important because they appear in many documents. One of the ways to solve this problem is stopwords filtering, but this solution is discrete and not flexible.\n",
        "\n",
        "TF-IDF (term frequency - inverse document frequency) can deal with this problem better. TF-IDF lowers the weight of commonly used words and raises the weight of rare words that occur only in current document. TF-IDF formula looks like this:\n",
        "<br><br>\n",
        "\n",
        "$tfidf(term, document)= tf(term, document) \\cdot idf(term)$\n",
        "\n",
        "<br>\n",
        "Where TF is calculated by dividing number of times the word occurs in the document by the total number of words in the document\n",
        "\n",
        "$tf(term, document)= \\frac{n_i}{\\sum_{k=1}^W n_k}$\n",
        "\n",
        "IDF (inverse document frequency), interpreted like inversed number of documents, in which the term we’re interested in occurs. N - number of documents, n(t) - number of documents with current word or term t.\n",
        "\n",
        "\n",
        "$idf(term) = \\log {\\frac{N}{n_t}} $\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRF9_B5lwURy"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n",
        "\n",
        "corpus = [\n",
        "          'Time flies like an arrow.',\n",
        "          'Fruit flies like a banana.'\n",
        "]\n",
        "\n",
        "vocab = ['an', 'arrow', 'banana', 'flies', 'fruit', 'like', 'time']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uckXLvR0DKIm"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
        "tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI8h6WGCDgY_"
      },
      "source": [
        "sns.heatmap(tfidf, annot=True, cbar = False, xticklabels = vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8IxcZQGTGb6"
      },
      "source": [
        "## Word2Vec and GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHdtizR4T1d_"
      },
      "source": [
        "The most commonly used models for word embeddings are [word2vec](https://github.com/dav/word2vec/) and [GloVe](https://nlp.stanford.edu/projects/glove/) which are both unsupervised approaches based on the distributional hypothesis (words that occur in the same contexts tend to have similar meanings).\n",
        "\n",
        "Word2Vec word embeddings are vector representations of words,\n",
        "that are typically learnt by an unsupervised model when fed\n",
        "with large amounts of text as input (e.g. Wikipedia, science, news, articles etc.). These representation of words capture semantic similarity between words among other properties. Word2Vec word embeddings are learnt in a such way, that [distance](https://en.wikipedia.org/wiki/Euclidean_distance) between vectors for words with close meanings (\"king\" and \"queen\" for example) are closer than distance for words with complety different meanings (\"king\" and \"carpet\" for example).\n",
        "\n",
        "![Замещающий текст](https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg)\n",
        "Image from [developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)\n",
        "\n",
        "Word2Vec vectors even allow some mathematic operations on vectors. For example, in this operation we are using word2vec vectors for each word:\n",
        "\n",
        "**king - man + woman = queen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEFSugKAFXeB"
      },
      "source": [
        "Another word embedding method is **Glove** (“Global Vectors”). It is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus. Then this matrix is factorized to a lower-dimensional (word x features) matrix, where each row now stores a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "wMk1XtkBS16q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27udJLwcbVLv"
      },
      "source": [
        "# Try Glove word embeddings with Spacy\n",
        "!python3 -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkURcwd5LJUp"
      },
      "source": [
        "import spacy\n",
        "# Load the spacy model that you have installed\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "# process a sentence using the model\n",
        "doc = nlp(\"man king stands on the carpet and sees woman queen\")\n",
        "# Get the vector for 'king':\n",
        "doc[1].vector[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(doc)):\n",
        "  print(doc[i])"
      ],
      "metadata": {
        "id": "P8Q0x1DqmE8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-998SUfAx-"
      },
      "source": [
        "Find similarity between King and Queen (higher value is better)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (i, dToken) in enumerate(doc):\n",
        "  print(i,dToken)"
      ],
      "metadata": {
        "id": "UrMvZ_gJ890Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K34HpkmWaEqV"
      },
      "source": [
        "doc[1].similarity(doc[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXm7aeh4hES5"
      },
      "source": [
        "Find similarity between King and carpet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSqwqBrUcwMB"
      },
      "source": [
        "doc[1].similarity(doc[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHsyuyS3h65C"
      },
      "source": [
        "Check if king - man + woman = queen. We will multiply vectors for 'man' and 'woman' by two, because subtracting the vector for 'man' and adding the vector for 'woman' will do little to the original vector for “king”, likely because those “man” and “woman” are related themselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXQ6uvkHh2bA"
      },
      "source": [
        "v =  doc[1].vector - (doc[0].vector*3) + (doc[8].vector*3)\n",
        "\n",
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "\n",
        "# Format the vocabulary for use in the distance function\n",
        "vectors = [token.vector for token in doc]\n",
        "vectors = np.array(vectors)\n",
        "\n",
        "# Find the closest word below\n",
        "closest_index = distance.cdist(np.expand_dims(v, axis = 0), vectors, metric = 'cosine').argmin()\n",
        "output_word = doc[closest_index].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjVLpbCG9829"
      },
      "source": [
        "output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uD3aN1wNDYM5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxnDjaFYMolw"
      },
      "source": [
        "## References\n",
        "\n",
        "- [BERT Word Embeddings Tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [The Illustrated GPT-2 (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/)\n",
        "- [FROM Pre-trained Word Embeddings TO Pre-trained Language Models — Focus on BERT](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)\n",
        "- [ Make your own Rick Sanchez (bot) with Transformers and DialoGPT fine-tuning](https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30)\n",
        "- [Playing with word vectors](https://medium.com/swlh/playing-with-word-vectors-308ab2faa519)\n",
        "- [Intuitive Guide to Understanding GloVe Embeddings](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)\n",
        "- [Word Embeddings in Python with Spacy and Gensim](https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/)\n",
        "- [Brief review of word embedding families (2019) ](https://medium.com/analytics-vidhya/brief-review-of-word-embedding-families-2019-b2bbc601bbfe)\n",
        "- [Word embeddings: exploration, explanation, and exploitation (with code in Python)](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)"
      ]
    }
  ]
}